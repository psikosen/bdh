# BDH Default Training Configuration
# Copy and modify for your experiments

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
model_type: hatching  # Options: bdh, hatching, advanced

# Architecture
n_layer: 6
n_embd: 256
n_head: 4
vocab_size: 50257  # GPT-2 tokenizer
dropout: 0.1
mlp_multiplier: 128

# Biological parameters (Hatching model only)
tau_mem: 10.0       # Membrane time constant (ms)
tau_syn: 5.0        # Synaptic time constant (ms)
tau_plus: 20.0      # STDP LTP time constant
v_threshold: 1.0    # Spike threshold

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
max_steps: 100000
batch_size: 32
gradient_accumulation_steps: 1
max_seq_len: 512

# =============================================================================
# OPTIMIZER CONFIGURATION
# =============================================================================
learning_rate: 3.0e-4
weight_decay: 0.1
beta1: 0.9
beta2: 0.95
max_grad_norm: 1.0

# Learning rate schedule
warmup_steps: 1000
lr_decay_steps: 100000
min_lr_ratio: 0.1

# =============================================================================
# CHECKPOINTING
# =============================================================================
checkpoint_dir: ./checkpoints
checkpoint_freq: 1000
keep_last_n_checkpoints: 5

# =============================================================================
# LOGGING
# =============================================================================
log_dir: ./logs
log_freq: 10
eval_freq: 500

# =============================================================================
# HARDWARE
# =============================================================================
device: auto        # auto, cuda, cpu, mps
dtype: bfloat16     # float32, float16, bfloat16
compile_model: true

# =============================================================================
# EXPERIMENT TRACKING
# =============================================================================
experiment_name: bdh_default
use_tensorboard: true
use_wandb: false
wandb_project: bdh

# =============================================================================
# EARLY STOPPING
# =============================================================================
early_stopping_patience: 0  # 0 = disabled
early_stopping_min_delta: 0.001

# =============================================================================
# CURRICULUM LEARNING (optional)
# =============================================================================
curriculum_stages: []
# Example:
# curriculum_stages:
#   - name: "stage1_basic"
#     max_steps: 10000
#     dataset: "tinystories"
#     learning_rate: 3.0e-4
#   - name: "stage2_complex"
#     max_steps: 50000
#     dataset: "wikitext"
#     learning_rate: 1.0e-4
